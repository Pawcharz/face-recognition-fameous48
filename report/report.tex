\documentclass[12pt]{article}

\usepackage[T1]{fontenc} % used for font (special symbols like _)
% \usepackage{listings} % could be used instead of minted
% \usepackage{xcolor} % used to define custom colors and use already defined ones
\usepackage{hyperref} % used for links in table of contents
\usepackage{tocbibind} % used for depth in table of contents
\usepackage[a4paper, margin=2cm]{geometry} % increase page margins
\usepackage{minted} % used for code highlighting
\usepackage{blindtext} % used for inserting lorem ipsum (\blindtext)
\usepackage[most]{tcolorbox} % used for notes
\usepackage{graphicx} % used for inserting images
% \usepackage{float} % used for fixing image in place
\usepackage{enumitem} % used for removing spaces between lines

\author{
  Jakub Czermański\\
  \texttt{193105}
  \and
  Paweł Blicharz\\
  \texttt{193193}
}
\date{02.05.2024}
\title{Using CNNs to classify people in Famous48 dataset - Subject 12c}

% Define how code should be displayed
% https://www.overleaf.com/learn/latex/Code_Highlighting_with_minted
% for minted, removes heading spaces: https://tex.stackexchange.com/questions/423275/minted-aligning-code-fragments-left

% remove spaces between items in lists:
\setlist{noitemsep}

\setcounter{tocdepth}{3} % set depth in the table of contents to 3
\hypersetup{
  colorlinks=true,
  allcolors=black,
  % linkcolor=blue,
  % filecolor=magenta,
  % urlcolor=cyan,
  % pdftitle={An Example},
  % pdfpagemode=FullScreen,
}

\makeatletter
\NewDocumentCommand{\mynote}{+O{}+m}{%
  \begingroup
  \tcbset{%
    noteshift/.store in=\mynote@shift,
    noteshift=1.5cm
  }
  \begin{tcolorbox}[nobeforeafter,
    enhanced,
    sharp corners,
    toprule=1pt,
    bottomrule=1pt,
    leftrule=0pt,
    rightrule=0pt,
    colback=yellow!20,
    #1,
    left skip=\mynote@shift,
    right skip=\mynote@shift,
    overlay={\node[right] (mynotenode) at ([xshift=-\mynote@shift]frame.west) {\textbf{Note:}} ;},
    ]
    #2
  \end{tcolorbox}
  \endgroup
  }
\makeatother


\begin{document}
  \pagenumbering{gobble}
  \maketitle
  \newpage
  \tableofcontents
  \newpage
  \pagenumbering{arabic}

  \section{Introduction}
    In this project we have 24x24 images of famous people and our goal is to create
    a model that can predict who the person is given the image. In order to do it,
    we divided our data into training, validation and test sets (validation set
    is created during learning - in fit() function). We use Convolutional Neural Networks
    and tested 2 popular CNN architectures: AlexNet and LeNet-5.
    \subsection*{Data description}
      famous48 is a set of example images contained faces of 48 famous persons like sportsmens, politicians, actors or television stars. It was divided into 3 files: x24x24.txt, y24x24.txt, z24x24.txt, each containing 16 personal classes.
      \\\\
      Attributes description:
      \begin{itemize}
        \item a1 - face containing flag: (1-with face, 0-without face)
        \item a2 - image number in current class (person) beginning from 0
        \item a3 - class (person) number beginning from 0
        \item a4 - sex (0 - woman, 1 - man)
        \item a5 - race (0- white, 1 - negro, 2 - indian, ...)
        \item a6 - age (0 - baby, 1 - young, 2 - middle-age, 3 - old)
        \item a7 - binokulars (0 - without, 1 - transparent, 2 - dark)
        \item a8 - emotional expression (not state!) (0 - sad, 1 - neutral, 2 - happy)
      \end{itemize}
    % asdf
    % \\\\
    \mynote{Full code can be found in \textit{notebook\_keras.ipynb, lenet.ipynb,
    alexnet.ipynb} and \textit{after\_optuna\_training.ipynb}.
    Due to limited space, we provide here only the most important code.}
  \section{Libraries used}
    Firstly, we import all our libraries:
    \inputminted[linenos]{python}{code/imports.py}
  \section{File loading}
    This is our code for loading files and we will not change it throughout our journey:
    \inputminted[linenos]{python}{code/loading.py}
  \section{Testing}
    Here are our all attempts at finding the best model. We tried out different architectures and hyperparameters to achieve that goal.
    \subsection{AlexNet}
      Firstly, we implemented AlexNet model (\url{https://papers.nips.cc/paper_files/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html})
      which won the 2012 ILFRC challenge. Here is our model in python code, after downscaling it and changing some parameters:
      \inputminted[linenos]{python}{code/alexnet.py}
      \paragraph{Hyperparameters \& methods used:}
        \begin{itemize}
          \item \textit{optimizer}: Adam, with \textit{learning rate} = 0.001
          \item \textit{epochs} = 250
          \item \textit{batch size} = 200
          \item \textit{validation set size} = 20\% of the training set
          \item \textit{shuffle} the training data before each epoch
          \item L2 regularization for convolutional layers: $l = 0.0006$
          \item L2 regularization for the dense layer: $l = 0.01$
        \end{itemize}
      \paragraph{Results:} (rounded to 3 decimal places)
        \begin{itemize}
          \item train set accuracy: 0.896
          \item train loss: 0.775
          \item validation set accuracy: 0.81
          \item validation set loss: 1.115
          \item test set accuracy: \textbf{0.8}
          \item test set loss: 1.137
        \end{itemize}
            \begin{figure}[H]
              \includegraphics[width=\linewidth]{images/alex-net.png}
              \caption{Model accuracy for AlexNet architecture - training \& validation sets}
              \label{fig:alex-net}
            \end{figure}
      \paragraph{Conclusions} \mbox{} \\
      We decided that 80\% is not enough so we did not tune hyperparameters.
    \subsection{LeNet-5}
      Next, we decided to used LeNet-5 architecture. a
      \subsubsection{Base model}
        \inputminted[linenos]{python}{code/lenet.py}
      \subsubsection{Testing hyperparameters}
        \paragraph{Attempt 1} \mbox{} \\\\
        We decided to stay with Adam optimizer and \textit{learning rate} = 0.001.
        We select regularization hyperparameters by hand (0.0006 and 0.01).
        \paragraph{Hyperparameters \& methods used:}
        \begin{itemize}
          \item \textit{optimizer}: Adam, with \textit{learning rate} = 0.001
          \item \textit{epochs} = 100
          \item \textit{batch size} = 200
          \item \textit{validation set size} = 20\% of the training set
          \item \textit{shuffle} the training data before each epoch
          \item L2 regularization for convolutional layers: $l = 0.0006$
          \item L2 regularization for the dense layer: $l = 0.01$
        \end{itemize}
        \paragraph{Results:}
          \begin{itemize}
            \item train set accuracy: 0.805
            \item train loss: 1.07
            \item validation set accuracy: 0.81
            \item validation set loss: 1.08
            \item test set accuracy: \textbf{0.794}
            \item test set loss: 1.095
          \end{itemize}
              \begin{figure}[H]
                \includegraphics[width=\linewidth]{images/lenet.png}
                \caption{Model accuracy for LeNet-5 architecture - training \& validation sets, 100 epochs}
                \label{fig:lenet}
              \end{figure}
        \paragraph{Conclusions} \mbox{} \\
          Results looked promising so we decided to test this model for 1000 epochs but unfortunately it
          started overfitting pretty soon and we achieved only \textbf{84.6\%} test set accuracy.
          Here is a graph representing results for the same model but for 1000 epochs:
          \begin{figure}[H]
            \includegraphics[width=\linewidth]{images/lenet1.png}
            \caption{Model accuracy for LeNet-5 architecture - training \& validation sets, 1000 epochs}
            \label{fig:lenet1}
          \end{figure}
        \paragraph{Attempt 2} \mbox{} \\
        \par We decided to find better hyperparameters ($l$) for regularization and
        we used optuna to do this (the values are presented below).
        \par We also introduced decaying learning rate and trained the model for 1000 epochs.
        This time we got much better results.
        \paragraph{Hyperparameters \& methods used:}
        \begin{itemize}
          \item \textit{optimizer}: Adam, with starting \textit{learning rate} = 0.001, and decreasing by 50\% each 150 epochs
          \item \textit{epochs} = 1000
          \item \textit{batch size} = 200
          \item \textit{validation set size} = 20\% of the training set
          \item \textit{shuffle} the training data before each epoch
          \item L2 regularization for convolutional layers: $l \approx 0.00061$
          \item L2 regularization for the dense layer: $l \approx 0.0301$
        \end{itemize}
        \paragraph{Results:}
          \begin{itemize}
            \item train set accuracy: 0.981
            \item train loss: 0.337
            \item validation set accuracy: 0.878
            \item validation set loss: 0.651
            \item test set accuracy: \textbf{0.892}
            \item test set loss: 0.629
          \end{itemize}
              \begin{figure}[H]
                \includegraphics[width=\linewidth]{images/lenet2.png}
                \caption{Model accuracy for LeNet-5 with optimized hyperparameters}
                \label{fig:lenet2}
              \end{figure}
        \paragraph{Conclusions} \mbox{} \\
        Hyperparameter tuning along with decaying learning rate helped
        us achieve higher prediction accuracy - from \textbf{84.6\%} in our first attempt
        to \textbf{89.2\%} in the second attempt. We are satisfied with the resulting model
        and its results.
    \section{Summary}
      To sum up, we used 2 different CNN architectures to train our models and
      while AlexNet performed pretty poorly, the LeNet, after some hyperparameter
      tuning, achieved very good results: \textbf{89.2\%} test set accuracy.


\end{document}
