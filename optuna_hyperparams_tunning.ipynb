{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the result reproducible\n",
    "keras.utils.set_random_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_devices = tf.config.list_physical_devices()\n",
    "print(physical_devices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.set_visible_devices(physical_devices[1], 'GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.constrains import CLASSES, IMAGE_SIZE\n",
    "\n",
    "BATCH_SIZE = 200\n",
    "EPOCHS = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers, regularizers, Sequential, Input\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.losses import categorical_crossentropy\n",
    "\n",
    "def define_model(trial):\n",
    "\n",
    "  conv_regularizer = regularizers.l2(l=trial.suggest_float(\"conv_regularizer\", 0.0001, 0.001)) # 0.0008\n",
    "  dense_regularizer = regularizers.l2(l=trial.suggest_float(\"dense_regularizer\", 0.005, 0.05)) # 0.01\n",
    "\n",
    "  dropout_base =trial.suggest_float(\"dropout_base\", 0, 0.4)\n",
    "  dropout_increment = trial.suggest_float(\"dropout_increment\", 0, 0.25)\n",
    "  activation_def = 'tanh' # trial.suggest_categorical(\"activation_def\", [\"tanh\", \"relu\"])\n",
    "  \n",
    "  model = Sequential(\n",
    "    [\n",
    "      Input(shape=(IMAGE_SIZE, IMAGE_SIZE, 1)),\n",
    "      layers.Conv2D(6, kernel_size=5, padding='same', activation=activation_def, kernel_regularizer=conv_regularizer),\n",
    "      layers.MaxPooling2D(pool_size=2, strides=2),\n",
    "      \n",
    "      layers.Dropout(dropout_base),\n",
    "      layers.Conv2D(16, kernel_size=5, padding='same', activation=activation_def, kernel_regularizer=conv_regularizer),\n",
    "      layers.MaxPooling2D(pool_size=2, strides=2),\n",
    "      \n",
    "      layers.Dropout(dropout_base + 1*dropout_increment),\n",
    "      layers.Conv2D(120, kernel_size=5, padding='same', activation=activation_def, kernel_regularizer=conv_regularizer),\n",
    "      \n",
    "      layers.Flatten(),\n",
    "      layers.Dropout(dropout_base + 2*dropout_increment),\n",
    "      layers.Dense(84, activation=activation_def, kernel_regularizer=dense_regularizer),\n",
    "      layers.Dense(CLASSES, activation='softmax'),\n",
    "    ]\n",
    "  )\n",
    "  \n",
    "  optimizer = Adam(learning_rate=0.001)\n",
    "\n",
    "  model.compile(loss=categorical_crossentropy, optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "  \n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defines training and evaluation.\n",
    "def train_model(model, X_train, y_train):\n",
    "  \n",
    "  history = model.fit(\n",
    "    X_train, y_train,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    validation_split=0.2,\n",
    "    shuffle=True,\n",
    "    verbose=0,\n",
    "  )\n",
    "  \n",
    "  return history\n",
    "\n",
    "def evaluate_trial(history):\n",
    "  MEASUREMENT_SPAN = 5\n",
    "  length = len(history.history['accuracy'])\n",
    "  \n",
    "  acc = np.mean(history.history['accuracy'][length-MEASUREMENT_SPAN:])\n",
    "  val_acc = np.mean(history.history['val_accuracy'][length-MEASUREMENT_SPAN:])\n",
    "\n",
    "  # Should be minimized\n",
    "  difference = acc - val_acc\n",
    "  \n",
    "  return val_acc, difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.load_dataset import load_dataset\n",
    "\n",
    "DIRPATH = './data'\n",
    "X_train, X_test, y_train, y_test = load_dataset(DIRPATH)\n",
    "\n",
    "def objective(trial):  \n",
    "  model = define_model(trial)\n",
    "  \n",
    "  history = train_model(model, X_train, y_train)\n",
    "  val_acc, difference = evaluate_trial(history)\n",
    "  return val_acc, difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study = optuna.create_study(directions=[\"maximize\", \"minimize\"])\n",
    "study.optimize(objective, n_trials=30, timeout=300)\n",
    "\n",
    "print(\"Number of finished trials: \", len(study.trials))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
