{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_devices = tf.config.list_physical_devices()\n",
    "tf.config.set_visible_devices(physical_devices[1], 'GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 200\n",
    "EPOCHS = 100\n",
    "# CHECKPOINT_PATH = 'models/checkpoints/famous48_optuna_temp_best.keras'\n",
    "\n",
    "CLASSES = 48\n",
    "IMAGE_SIZE = 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "\n",
    "def load_dataset():\n",
    "\n",
    "  def read_file(filename):\n",
    "  \n",
    "    with open(filename, 'r') as file:\n",
    "      lines = file.readlines()\n",
    "      \n",
    "      # Remove newLines\n",
    "      for i, line in enumerate(lines):\n",
    "        lines[i] = line.replace('\\n', '')\n",
    "      \n",
    "      # We assume these are integers\n",
    "      EXAMPLES_NR = int(lines[0])\n",
    "      PIXELS_NR = int(lines[1])\n",
    "      \n",
    "      inputs = list()\n",
    "      labels = np.zeros(EXAMPLES_NR, dtype=int)\n",
    "      \n",
    "      examples_raw = lines[2:EXAMPLES_NR+2]\n",
    "      \n",
    "      for i, example_raw in enumerate(examples_raw):\n",
    "        # Split by spaces (treats multiple as one)\n",
    "        tokens = re.split('\\s+', example_raw)\n",
    "        \n",
    "        pixel_values = np.array(tokens[0:PIXELS_NR])\n",
    "        attributes = tokens[PIXELS_NR:]\n",
    "        \n",
    "        pixel_values = np.array(pixel_values, dtype=float)\n",
    "        pixel_values = pixel_values.reshape([IMAGE_SIZE, IMAGE_SIZE])\n",
    "        \n",
    "        inputs.append(pixel_values)\n",
    "        labels[i] = int(attributes[2])\n",
    "        \n",
    "      inputs = np.array(inputs)\n",
    "    return inputs, labels\n",
    "\n",
    "  # classes 0-15\n",
    "  X_0, y_0 = read_file('./data/x24x24.txt')\n",
    "  # classes 16-31\n",
    "  X_1, y_1 = read_file('./data/y24x24.txt')\n",
    "  # # classes 32-48\n",
    "  X_2, y_2 = read_file('./data/z24x24.txt')\n",
    "  \n",
    "  # Concatenate train and test images\n",
    "  X = np.concatenate((X_0, X_1, X_2))\n",
    "  y = np.concatenate((y_0, y_1, y_2))\n",
    "  print(len(X), len(y))\n",
    "\n",
    "  N_TRAIN_EXAMPLES=int(len(X) * 0.8)\n",
    "  N_TEST_EXAMPLES=len(X) - N_TRAIN_EXAMPLES\n",
    "\n",
    "\n",
    "  X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=N_TRAIN_EXAMPLES, test_size=N_TEST_EXAMPLES, random_state=1)\n",
    "\n",
    "  # One-Hot encoding\n",
    "  # Getting dummy variables\n",
    "  y_train_fixed = np.zeros((y_train.shape[0], CLASSES))\n",
    "  y_test_fixed = np.zeros((y_test.shape[0], CLASSES))\n",
    "\n",
    "  for i, value in enumerate(y_train):\n",
    "    y_train_fixed[i][value] = 1\n",
    "    \n",
    "  for i, value in enumerate(y_test):\n",
    "    y_test_fixed[i][value] = 1\n",
    "    \n",
    "  return X_train, X_test, y_train_fixed, y_test_fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers, regularizers, Sequential, Input\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.losses import categorical_crossentropy\n",
    "\n",
    "def define_model(trial):\n",
    "\n",
    "  conv_regularizer = regularizers.l2(l=trial.suggest_float(\"conv_regularizer\", 0.001, 0.0001)) # 0.0008\n",
    "  dense_regularizer = regularizers.l2(l=trial.suggest_float(\"dense_regularizer\", 0.005, 0.05)) # 0.01\n",
    "\n",
    "  dropout_base = regularizers.l2(l=trial.suggest_float(\"dropout_base\", 0, 0.4))\n",
    "  dropout_increment = regularizers.l2(l=trial.suggest_float(\"dropout_increment\", 0, 0.25))\n",
    "  activation_def = trial.suggest_categorical(\"activation_def\", [\"tanh\", \"relu\"])\n",
    "  \n",
    "  model = Sequential(\n",
    "    [\n",
    "      Input(shape=(IMAGE_SIZE, IMAGE_SIZE, 1)),\n",
    "      layers.Conv2D(6, kernel_size=5, padding='same', activation=activation_def, kernel_regularizer=conv_regularizer),\n",
    "      layers.MaxPooling2D(pool_size=2, strides=2),\n",
    "      \n",
    "      layers.Dropout(dropout_base),\n",
    "      layers.Conv2D(16, kernel_size=5, padding='same', activation=activation_def, kernel_regularizer=conv_regularizer),\n",
    "      layers.MaxPooling2D(pool_size=2, strides=2),\n",
    "      \n",
    "      layers.Dropout(dropout_base + 1*dropout_increment),\n",
    "      layers.Conv2D(120, kernel_size=5, padding='same', activation=activation_def, kernel_regularizer=conv_regularizer),\n",
    "      \n",
    "      layers.Flatten(),\n",
    "      layers.Dropout(dropout_base + 2*dropout_increment),\n",
    "      layers.Dense(84, activation=activation_def, kernel_regularizer=dense_regularizer),\n",
    "      layers.Dense(CLASSES, activation='softmax'),\n",
    "    ]\n",
    "  )\n",
    "  \n",
    "  optimizer = Adam(learning_rate=0.001)\n",
    "\n",
    "  model.compile(loss=categorical_crossentropy, optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "  \n",
    "  return model\n",
    "\n",
    "\n",
    "# Defines training and evaluation.\n",
    "def train_model(model, X_train, y_train):\n",
    "  \n",
    "  history = model.fit(\n",
    "    X_train, y_train,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    validation_split=0.2,\n",
    "    shuffle=True,\n",
    "    verbose=0,\n",
    "  )\n",
    "  \n",
    "  return history\n",
    "\n",
    "\n",
    "def evaluate_trial(history):\n",
    "  MEASUREMENT_SPAN = 5\n",
    "  length = len(history.history['accuracy'])\n",
    "  \n",
    "  acc = np.mean(history.history['accuracy'][length-MEASUREMENT_SPAN:])\n",
    "  val_acc = np.mean(history.history['val_accuracy'][length-MEASUREMENT_SPAN:])\n",
    "\n",
    "  # Should be minimized\n",
    "  difference = acc - val_acc\n",
    "  \n",
    "  return val_acc, difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = load_dataset()\n",
    "\n",
    "def objective(trial):  \n",
    "  model = define_model(trial)\n",
    "  \n",
    "  history = train_model(model, X_train, y_train)\n",
    "  val_acc, difference = evaluate_trial(history)\n",
    "  return val_acc, difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study = optuna.create_study(directions=[\"maximize\", \"minimize\"])\n",
    "study.optimize(objective, n_trials=30, timeout=3600)\n",
    "\n",
    "print(\"Number of finished trials: \", len(study.trials))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
